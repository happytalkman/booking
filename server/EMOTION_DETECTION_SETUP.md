# ğŸ­ ì–¼êµ´ í‘œì • ì¸ì‹ ì„œë²„ ì„¤ì¹˜ ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

FER (Facial Expression Recognition) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ ì–¼êµ´ í‘œì • ì¸ì‹ ì„œë²„ì…ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥
- 7ê°€ì§€ ê°ì • ì¸ì‹ (í–‰ë³µ, ìŠ¬í””, í™”ë‚¨, ë†€ëŒ, ë‘ë ¤ì›€, í˜ì˜¤, ì¤‘ë¦½)
- MTCNN ê¸°ë°˜ ì–¼êµ´ ê²€ì¶œ
- CNN ëª¨ë¸ ê¸°ë°˜ í‘œì • ë¶„ë¥˜
- REST API ì œê³µ

---

## ğŸ”§ ì„¤ì¹˜ ë°©ë²•

### 1. Python ì„¤ì¹˜ í™•ì¸
```bash
python --version  # Python 3.8 ì´ìƒ í•„ìš”
```

### 2. ê°€ìƒí™˜ê²½ ìƒì„± (ê¶Œì¥)
```bash
cd server
python -m venv venv

# Windows
venv\Scripts\activate

# Mac/Linux
source venv/bin/activate
```

### 3. íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
pip install -r requirements.txt
```

**ì„¤ì¹˜ë˜ëŠ” íŒ¨í‚¤ì§€:**
- `flask`: ì›¹ ì„œë²„
- `flask-cors`: CORS ì§€ì›
- `opencv-python`: ì´ë¯¸ì§€ ì²˜ë¦¬
- `fer`: ì–¼êµ´ í‘œì • ì¸ì‹
- `tensorflow`: ë”¥ëŸ¬ë‹ ëª¨ë¸
- `mtcnn`: ì–¼êµ´ ê²€ì¶œ
- `numpy`, `Pillow`: ì´ë¯¸ì§€ ì²˜ë¦¬

---

## ğŸš€ ì„œë²„ ì‹¤í–‰

### 1. ì„œë²„ ì‹œì‘
```bash
python emotion_detection.py
```

### 2. ì„œë²„ í™•ì¸
```
ğŸ­ Emotion Detection Server Starting...
ğŸ“ Server: http://localhost:5000
ğŸ”§ Model: FER (Facial Expression Recognition)
âœ… Ready to analyze emotions!
```

### 3. ìƒíƒœ í™•ì¸
ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†:
```
http://localhost:5000/api/health
```

ì‘ë‹µ:
```json
{
  "status": "ok",
  "message": "Emotion detection server is running",
  "model": "FER with MTCNN"
}
```

---

## ğŸ“¡ API ì‚¬ìš©ë²•

### Endpoint: POST /api/analyze-emotion

**Request:**
```json
{
  "image": "data:image/jpeg;base64,/9j/4AAQSkZJRg..."
}
```

**Response (ì„±ê³µ):**
```json
{
  "success": true,
  "emotion": "positive",
  "confidence": 0.85,
  "facial_score": 0.72,
  "details": {
    "dominant_emotion": "happy",
    "dominant_emotion_ko": "í–‰ë³µ",
    "dominant_score": 0.85,
    "all_emotions": {
      "happy": 0.85,
      "neutral": 0.10,
      "sad": 0.03,
      "angry": 0.01,
      "surprise": 0.01,
      "fear": 0.00,
      "disgust": 0.00
    },
    "positive_score": 0.86,
    "negative_score": 0.04,
    "neutral_score": 0.10
  },
  "analysis": {
    "ko": "ë¶„ì„ ê²°ê³¼, í˜„ì¬ ê¸ì •ì ì¸ ê°ì • ìƒíƒœì…ë‹ˆë‹¤...",
    "en": "Analysis shows you are in a positive emotional state..."
  }
}
```

**Response (ì–¼êµ´ ë¯¸ê²€ì¶œ):**
```json
{
  "error": "No face detected",
  "message": "ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¹´ë©”ë¼ë¥¼ ì •ë©´ìœ¼ë¡œ í–¥í•´ì£¼ì„¸ìš”."
}
```

---

## ğŸ§  ê°ì • ë¶„ì„ ì•Œê³ ë¦¬ì¦˜

### 1. ì–¼êµ´ ê²€ì¶œ
- **MTCNN** (Multi-task Cascaded Convolutional Networks)
- ì–¼êµ´ ì˜ì—­ ìë™ ê²€ì¶œ
- ì–¼êµ´ ëœë“œë§ˆí¬ ì¶”ì¶œ

### 2. í‘œì • ë¶„ë¥˜
- **CNN ëª¨ë¸** (FER2013 ë°ì´í„°ì…‹ í•™ìŠµ)
- 7ê°€ì§€ ê°ì • ë¶„ë¥˜
- ê° ê°ì •ë³„ í™•ë¥  ì¶œë ¥ (0-1)

### 3. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
```python
ê¸ì • (positive):
  - happy (í–‰ë³µ)
  - surprise (ë†€ëŒ)

ì¤‘ë¦½ (neutral):
  - neutral (ì¤‘ë¦½)

ë¶€ì • (negative):
  - sad (ìŠ¬í””)
  - angry (í™”ë‚¨)
  - fear (ë‘ë ¤ì›€)
  - disgust (í˜ì˜¤)
```

### 4. ì ìˆ˜ ê³„ì‚°
```python
facial_score = (positive_score - negative_score + 1) / 2
# ë²”ìœ„: 0-1
# 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê¸ì •ì 
```

---

## ğŸ¯ í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™

### 1. ê°ì • ê°ì§€ ë²„íŠ¼ í´ë¦­
```typescript
// VoiceQnAPanel.tsx
<button onClick={() => setShowEmotionModal(true)}>
  ê°ì • ê°ì§€
</button>
```

### 2. ì¹´ë©”ë¼ í™œì„±í™”
```typescript
// EmotionDetectionModal.tsx
const stream = await navigator.mediaDevices.getUserMedia({ 
  video: true 
});
```

### 3. ì´ë¯¸ì§€ ìº¡ì²˜
```typescript
const canvas = canvasRef.current;
const video = videoRef.current;
ctx.drawImage(video, 0, 0);
const imageData = canvas.toDataURL('image/jpeg');
```

### 4. API í˜¸ì¶œ
```typescript
const response = await fetch('http://localhost:5000/api/analyze-emotion', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ image: imageData })
});
```

### 5. ê²°ê³¼ í‘œì‹œ
```typescript
const result = await response.json();
setEmotionResult({
  emotion: result.emotion,
  confidence: result.confidence,
  details: result.details
});
```

---

## ğŸ” ë¬¸ì œ í•´ê²°

### 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì˜¤ë¥˜
```bash
# TensorFlow ì„¤ì¹˜ ì‹¤íŒ¨ ì‹œ
pip install tensorflow==2.15.0 --no-cache-dir

# OpenCV ì„¤ì¹˜ ì‹¤íŒ¨ ì‹œ
pip install opencv-python-headless
```

### 2. CORS ì˜¤ë¥˜
```python
# emotion_detection.pyì—ì„œ CORS ì„¤ì • í™•ì¸
CORS(app)  # ëª¨ë“  ë„ë©”ì¸ í—ˆìš©
```

### 3. ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨
- ì¡°ëª… í™•ì¸ (ë°ì€ í™˜ê²½)
- ì¹´ë©”ë¼ ì •ë©´ ì‘ì‹œ
- ì–¼êµ´ ì „ì²´ê°€ í™”ë©´ì— ë‚˜ì˜¤ë„ë¡

### 4. ì„œë²„ í¬íŠ¸ ì¶©ëŒ
```python
# ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
app.run(host='0.0.0.0', port=5001, debug=True)
```

---

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### 1. ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
```typescript
// 640x480ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
canvas.width = 640;
canvas.height = 480;
```

### 2. JPEG í’ˆì§ˆ ì¡°ì •
```typescript
// í’ˆì§ˆ 80%ë¡œ ì••ì¶•
canvas.toDataURL('image/jpeg', 0.8);
```

### 3. ìºì‹±
```python
# ëª¨ë¸ í•œ ë²ˆë§Œ ë¡œë“œ
detector = FER(mtcnn=True)  # ì „ì—­ ë³€ìˆ˜
```

---

## ğŸš€ í”„ë¡œë•ì…˜ ë°°í¬

### 1. Gunicorn ì‚¬ìš©
```bash
pip install gunicorn
gunicorn -w 4 -b 0.0.0.0:5000 emotion_detection:app
```

### 2. Docker ì»¨í…Œì´ë„ˆ
```dockerfile
FROM python:3.9
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY emotion_detection.py .
CMD ["python", "emotion_detection.py"]
```

### 3. í™˜ê²½ ë³€ìˆ˜
```bash
export FLASK_ENV=production
export FLASK_DEBUG=0
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **FER ë¼ì´ë¸ŒëŸ¬ë¦¬**: https://github.com/justinshenk/fer
- **MTCNN**: https://github.com/ipazc/mtcnn
- **FER2013 ë°ì´í„°ì…‹**: https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge
- **OpenCV**: https://opencv.org/

---

## ğŸ“ í–¥í›„ ê°œì„  ë°©í–¥

### ë‹¨ê¸°
- [ ] ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„
- [ ] ë‹¤ì¤‘ ì–¼êµ´ ë™ì‹œ ë¶„ì„
- [ ] ê°ì • ì¶”ì„¸ ê·¸ë˜í”„

### ì¤‘ê¸°
- [ ] ë”¥ëŸ¬ë‹ ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ
- [ ] ê°œì¸í™”ëœ ê°ì • í”„ë¡œí•„
- [ ] ìŒì„± í†¤ ë¶„ì„ í†µí•©

### ì¥ê¸°
- [ ] ë©€í‹°ëª¨ë‹¬ ê°ì • ì¸ì‹
- [ ] ê°ì • ì˜ˆì¸¡ ëª¨ë¸
- [ ] í´ë¼ìš°ë“œ ë°°í¬

---

**ê°œë°œì**: Kiro AI Assistant  
**ë²„ì „**: 1.0.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2024-12-09
