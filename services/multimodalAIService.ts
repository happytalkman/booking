// ë©€í‹°ëª¨ë‹¬ AI ì„œë¹„ìŠ¤
// ìŒì„±, ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ë¥¼ í†µí•© ì²˜ë¦¬í•˜ëŠ” ê³ ê¸‰ AI ì–´ì‹œìŠ¤í„´íŠ¸

interface MultimodalInput {
  text?: string;
  audio?: Blob;
  image?: File;
  context?: string;
  language?: 'ko' | 'en';
}

interface MultimodalResponse {
  text: string;
  audio?: Blob;
  confidence: number;
  modalities: string[];
  insights: AIInsight[];
  actions: SuggestedAction[];
}

interface AIInsight {
  type: 'document_analysis' | 'voice_command' | 'image_recognition' | 'context_understanding';
  content: string;
  confidence: number;
  metadata?: any;
}

interface SuggestedAction {
  id: string;
  type: 'booking' | 'query' | 'analysis' | 'navigation';
  description: string;
  parameters: any;
  priority: 'high' | 'medium' | 'low';
}

class MultimodalAIService {
  private speechRecognition: SpeechRecognition | null = null;
  private speechSynthesis: SpeechSynthesis;
  private isListening = false;
  private audioContext: AudioContext | null = null;

  constructor() {
    this.initializeSpeechServices();
    this.speechSynthesis = window.speechSynthesis;
  }

  // ìŒì„± ì¸ì‹ ì´ˆê¸°í™”
  private initializeSpeechServices() {
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      this.speechRecognition = new SpeechRecognition();
      
      this.speechRecognition.continuous = true;
      this.speechRecognition.interimResults = true;
      this.speechRecognition.lang = 'ko-KR';
    }
  }

  // ë©€í‹°ëª¨ë‹¬ ì…ë ¥ ì²˜ë¦¬
  async processMultimodalInput(input: MultimodalInput): Promise<MultimodalResponse> {
    const insights: AIInsight[] = [];
    const actions: SuggestedAction[] = [];
    let combinedText = input.text || '';
    let confidence = 0.8;

    try {
      // 1. ìŒì„± ì²˜ë¦¬
      if (input.audio) {
        const audioInsight = await this.processAudio(input.audio, input.language);
        insights.push(audioInsight);
        combinedText += ' ' + audioInsight.content;
      }

      // 2. ì´ë¯¸ì§€ ì²˜ë¦¬
      if (input.image) {
        const imageInsight = await this.processImage(input.image);
        insights.push(imageInsight);
        combinedText += ' ' + imageInsight.content;
      }

      // 3. í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ì»¨í…ìŠ¤íŠ¸ ì´í•´
      if (combinedText.trim()) {
        const textInsight = await this.processText(combinedText, input.context);
        insights.push(textInsight);
      }

      // 4. í†µí•© ë¶„ì„ ë° ì•¡ì…˜ ìƒì„±
      const integratedAnalysis = await this.integrateInsights(insights, input.context);
      actions.push(...integratedAnalysis.actions);

      // 5. ì‘ë‹µ ìƒì„±
      const responseText = await this.generateResponse(insights, actions, input.language);

      return {
        text: responseText,
        confidence: Math.min(confidence, 0.95),
        modalities: this.getUsedModalities(input),
        insights,
        actions
      };

    } catch (error) {
      console.error('Multimodal processing error:', error);
      return {
        text: input.language === 'ko' ? 
          'ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' : 
          'Sorry, an error occurred while processing your request.',
        confidence: 0.1,
        modalities: [],
        insights: [],
        actions: []
      };
    }
  }

  // ìŒì„± ì²˜ë¦¬
  private async processAudio(audio: Blob, language: 'ko' | 'en' = 'ko'): Promise<AIInsight> {
    return new Promise((resolve) => {
      if (!this.speechRecognition) {
        resolve({
          type: 'voice_command',
          content: 'ìŒì„± ì¸ì‹ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.',
          confidence: 0.1
        });
        return;
      }

      this.speechRecognition.lang = language === 'ko' ? 'ko-KR' : 'en-US';
      
      this.speechRecognition.onresult = (event) => {
        const transcript = event.results[event.results.length - 1][0].transcript;
        const confidence = event.results[event.results.length - 1][0].confidence;
        
        resolve({
          type: 'voice_command',
          content: transcript,
          confidence: confidence || 0.8,
          metadata: { language, duration: audio.size }
        });
      };

      this.speechRecognition.onerror = () => {
        resolve({
          type: 'voice_command',
          content: 'ìŒì„± ì¸ì‹ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.',
          confidence: 0.1
        });
      };

      // ì‹¤ì œë¡œëŠ” audio blobì„ ì²˜ë¦¬í•´ì•¼ í•˜ì§€ë§Œ, 
      // ë¸Œë¼ìš°ì € ì œí•œìœ¼ë¡œ ì¸í•´ ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ìœ¼ë¡œ ëŒ€ì²´
      this.speechRecognition.start();
      
      setTimeout(() => {
        this.speechRecognition?.stop();
      }, 5000);
    });
  }

  // ì´ë¯¸ì§€ ì²˜ë¦¬ (OCR ë° ê°ì²´ ì¸ì‹)
  private async processImage(image: File): Promise<AIInsight> {
    try {
      // ì´ë¯¸ì§€ë¥¼ Canvasë¡œ ë¡œë“œí•˜ì—¬ ë¶„ì„
      const canvas = document.createElement('canvas');
      const ctx = canvas.getContext('2d');
      const img = new Image();

      return new Promise((resolve) => {
        img.onload = async () => {
          canvas.width = img.width;
          canvas.height = img.height;
          ctx?.drawImage(img, 0, 0);

          // ê°„ë‹¨í•œ ì´ë¯¸ì§€ ë¶„ì„ (ì‹¤ì œë¡œëŠ” OCR APIë‚˜ Vision API ì‚¬ìš©)
          const analysis = await this.analyzeImageContent(canvas);
          
          resolve({
            type: 'image_recognition',
            content: analysis.text,
            confidence: analysis.confidence,
            metadata: {
              width: img.width,
              height: img.height,
              size: image.size,
              type: image.type,
              objects: analysis.objects
            }
          });
        };

        img.onerror = () => {
          resolve({
            type: 'image_recognition',
            content: 'ì´ë¯¸ì§€ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.',
            confidence: 0.1
          });
        };

        img.src = URL.createObjectURL(image);
      });

    } catch (error) {
      return {
        type: 'image_recognition',
        content: 'ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
        confidence: 0.1
      };
    }
  }

  // ì´ë¯¸ì§€ ë‚´ìš© ë¶„ì„
  private async analyzeImageContent(canvas: HTMLCanvasElement): Promise<{
    text: string;
    confidence: number;
    objects: string[];
  }> {
    // ì‹¤ì œë¡œëŠ” Google Vision API, AWS Rekognition, ë˜ëŠ” Tesseract.js ì‚¬ìš©
    // ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ ë¶„ì„ ê²°ê³¼ ë°˜í™˜
    
    const imageData = canvas.getContext('2d')?.getImageData(0, 0, canvas.width, canvas.height);
    if (!imageData) {
      return { text: 'ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', confidence: 0.1, objects: [] };
    }

    // ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ë¬¸ì„œ ìœ í˜• ê°ì§€
    const patterns = this.detectDocumentPatterns(imageData);
    
    if (patterns.includes('bill_of_lading')) {
      return {
        text: 'ì„ í•˜ì¦ê¶Œ(B/L) ë¬¸ì„œê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. í™”ë¬¼ ì •ë³´ì™€ ìš´ì†¡ ì¡°ê±´ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
        confidence: 0.85,
        objects: ['bill_of_lading', 'shipping_document', 'container_info']
      };
    }

    if (patterns.includes('commercial_invoice')) {
      return {
        text: 'ìƒì—…ì†¡ì¥(Commercial Invoice) ë¬¸ì„œê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. í™”ë¬¼ ê°€ì¹˜ì™€ ì„¸ë¶€ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
        confidence: 0.82,
        objects: ['commercial_invoice', 'cargo_details', 'value_declaration']
      };
    }

    if (patterns.includes('container_image')) {
      return {
        text: 'ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì»¨í…Œì´ë„ˆ ë²ˆí˜¸ì™€ ìƒíƒœë¥¼ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
        confidence: 0.78,
        objects: ['container', 'container_number', 'physical_condition']
      };
    }

    return {
      text: 'ì¼ë°˜ ì´ë¯¸ì§€ì…ë‹ˆë‹¤. ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.',
      confidence: 0.6,
      objects: ['general_image']
    };
  }

  // ë¬¸ì„œ íŒ¨í„´ ê°ì§€
  private detectDocumentPatterns(imageData: ImageData): string[] {
    const patterns: string[] = [];
    
    // ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒ¨í„´ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
    // ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜
    const randomValue = Math.random();
    
    if (randomValue > 0.7) patterns.push('bill_of_lading');
    else if (randomValue > 0.4) patterns.push('commercial_invoice');
    else if (randomValue > 0.2) patterns.push('container_image');
    
    return patterns;
  }

  // í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ì»¨í…ìŠ¤íŠ¸ ì´í•´
  private async processText(text: string, context?: string): Promise<AIInsight> {
    try {
      // ì˜ë„ ë¶„ì„
      const intent = this.analyzeIntent(text);
      
      // ì—”í‹°í‹° ì¶”ì¶œ
      const entities = this.extractEntities(text);
      
      // ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì´í•´
      const contextualUnderstanding = this.analyzeContext(text, context, entities);

      return {
        type: 'context_understanding',
        content: contextualUnderstanding.summary,
        confidence: contextualUnderstanding.confidence,
        metadata: {
          intent,
          entities,
          context: contextualUnderstanding.context
        }
      };

    } catch (error) {
      return {
        type: 'context_understanding',
        content: 'í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
        confidence: 0.1
      };
    }
  }

  // ì˜ë„ ë¶„ì„
  private analyzeIntent(text: string): string {
    const lowerText = text.toLowerCase();
    
    if (lowerText.includes('ë¶€í‚¹') || lowerText.includes('booking') || lowerText.includes('ì˜ˆì•½')) {
      return 'booking_inquiry';
    }
    if (lowerText.includes('ìš´ì„') || lowerText.includes('rate') || lowerText.includes('ê°€ê²©')) {
      return 'rate_inquiry';
    }
    if (lowerText.includes('ì¶”ì ') || lowerText.includes('track') || lowerText.includes('ìœ„ì¹˜')) {
      return 'tracking_inquiry';
    }
    if (lowerText.includes('ì¼ì •') || lowerText.includes('schedule') || lowerText.includes('ì‹œê°„')) {
      return 'schedule_inquiry';
    }
    if (lowerText.includes('ë¶„ì„') || lowerText.includes('analysis') || lowerText.includes('ë¦¬í¬íŠ¸')) {
      return 'analysis_request';
    }
    
    return 'general_inquiry';
  }

  // ì—”í‹°í‹° ì¶”ì¶œ
  private extractEntities(text: string): any {
    const entities: any = {};
    
    // í•­êµ¬ ì´ë¦„ ì¶”ì¶œ
    const ports = ['ë¶€ì‚°', 'ì¸ì²œ', 'ìš¸ì‚°', 'busan', 'incheon', 'ulsan', 'LA', 'long beach', 'shanghai'];
    ports.forEach(port => {
      if (text.toLowerCase().includes(port.toLowerCase())) {
        entities.ports = entities.ports || [];
        entities.ports.push(port);
      }
    });

    // ë‚ ì§œ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´)
    const datePattern = /\d{4}-\d{2}-\d{2}|\d{2}\/\d{2}\/\d{4}/g;
    const dates = text.match(datePattern);
    if (dates) entities.dates = dates;

    // ì»¨í…Œì´ë„ˆ ë²ˆí˜¸ ì¶”ì¶œ
    const containerPattern = /[A-Z]{4}\d{7}/g;
    const containers = text.match(containerPattern);
    if (containers) entities.containers = containers;

    // í™”í ê¸ˆì•¡ ì¶”ì¶œ
    const currencyPattern = /\$[\d,]+|\â‚©[\d,]+|USD\s*[\d,]+|KRW\s*[\d,]+/g;
    const amounts = text.match(currencyPattern);
    if (amounts) entities.amounts = amounts;

    return entities;
  }

  // ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
  private analyzeContext(text: string, context?: string, entities?: any): {
    summary: string;
    confidence: number;
    context: any;
  } {
    let summary = '';
    let confidence = 0.7;
    const contextData: any = { entities };

    if (context === 'booking_analysis') {
      summary = 'ë¶€í‚¹ ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. ';
      confidence += 0.1;
    } else if (context === 'market_intel') {
      summary = 'ì‹œì¥ ì¸í…”ë¦¬ì „ìŠ¤ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. ';
      confidence += 0.1;
    }

    if (entities?.ports?.length > 0) {
      summary += `${entities.ports.join(', ')} í•­êµ¬ì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. `;
      confidence += 0.05;
    }

    if (entities?.dates?.length > 0) {
      summary += `${entities.dates.join(', ')} ë‚ ì§œ ì •ë³´ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. `;
      confidence += 0.05;
    }

    if (entities?.containers?.length > 0) {
      summary += `${entities.containers.join(', ')} ì»¨í…Œì´ë„ˆ ë²ˆí˜¸ë¥¼ ì‹ë³„í–ˆìŠµë‹ˆë‹¤. `;
      confidence += 0.1;
    }

    return {
      summary: summary || 'ì¼ë°˜ì ì¸ í•´ìš´ ë¬¼ë¥˜ ê´€ë ¨ ìš”ì²­ìœ¼ë¡œ ì´í•´í–ˆìŠµë‹ˆë‹¤.',
      confidence: Math.min(confidence, 0.95),
      context: contextData
    };
  }

  // ì¸ì‚¬ì´íŠ¸ í†µí•© ë° ì•¡ì…˜ ìƒì„±
  private async integrateInsights(insights: AIInsight[], context?: string): Promise<{
    actions: SuggestedAction[];
  }> {
    const actions: SuggestedAction[] = [];

    insights.forEach((insight, index) => {
      switch (insight.type) {
        case 'voice_command':
          if (insight.content.includes('ë¶€í‚¹') || insight.content.includes('booking')) {
            actions.push({
              id: `voice_booking_${index}`,
              type: 'booking',
              description: 'ìŒì„±ìœ¼ë¡œ ìš”ì²­ëœ ë¶€í‚¹ ì¡°íšŒë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.',
              parameters: { query: insight.content, source: 'voice' },
              priority: 'high'
            });
          }
          break;

        case 'image_recognition':
          if (insight.metadata?.objects?.includes('bill_of_lading')) {
            actions.push({
              id: `image_bl_${index}`,
              type: 'analysis',
              description: 'ì„ í•˜ì¦ê¶Œ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ë¶€í‚¹ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.',
              parameters: { documentType: 'bill_of_lading', confidence: insight.confidence },
              priority: 'high'
            });
          }
          break;

        case 'context_understanding':
          const intent = insight.metadata?.intent;
          if (intent === 'rate_inquiry') {
            actions.push({
              id: `context_rate_${index}`,
              type: 'query',
              description: 'ìš´ì„ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.',
              parameters: { 
                intent, 
                entities: insight.metadata?.entities,
                context 
              },
              priority: 'medium'
            });
          }
          break;
      }
    });

    return { actions };
  }

  // ì‘ë‹µ ìƒì„±
  private async generateResponse(
    insights: AIInsight[], 
    actions: SuggestedAction[], 
    language: 'ko' | 'en' = 'ko'
  ): Promise<string> {
    let response = '';

    if (language === 'ko') {
      response = 'ë©€í‹°ëª¨ë‹¬ ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.\n\n';
      
      if (insights.length > 0) {
        response += 'ğŸ“Š ë¶„ì„ ê²°ê³¼:\n';
        insights.forEach((insight, index) => {
          const typeLabel = this.getInsightTypeLabel(insight.type, language);
          response += `${index + 1}. ${typeLabel}: ${insight.content}\n`;
        });
        response += '\n';
      }

      if (actions.length > 0) {
        response += 'ğŸ¯ ì¶”ì²œ ì•¡ì…˜:\n';
        actions.forEach((action, index) => {
          response += `${index + 1}. ${action.description}\n`;
        });
      }
    } else {
      response = 'Multimodal analysis completed.\n\n';
      
      if (insights.length > 0) {
        response += 'ğŸ“Š Analysis Results:\n';
        insights.forEach((insight, index) => {
          const typeLabel = this.getInsightTypeLabel(insight.type, language);
          response += `${index + 1}. ${typeLabel}: ${insight.content}\n`;
        });
        response += '\n';
      }

      if (actions.length > 0) {
        response += 'ğŸ¯ Suggested Actions:\n';
        actions.forEach((action, index) => {
          response += `${index + 1}. ${action.description}\n`;
        });
      }
    }

    return response;
  }

  // ì¸ì‚¬ì´íŠ¸ íƒ€ì… ë¼ë²¨
  private getInsightTypeLabel(type: string, language: 'ko' | 'en'): string {
    const labels = {
      ko: {
        voice_command: 'ìŒì„± ëª…ë ¹',
        image_recognition: 'ì´ë¯¸ì§€ ì¸ì‹',
        document_analysis: 'ë¬¸ì„œ ë¶„ì„',
        context_understanding: 'ì»¨í…ìŠ¤íŠ¸ ì´í•´'
      },
      en: {
        voice_command: 'Voice Command',
        image_recognition: 'Image Recognition',
        document_analysis: 'Document Analysis',
        context_understanding: 'Context Understanding'
      }
    };

    return labels[language][type as keyof typeof labels.ko] || type;
  }

  // ì‚¬ìš©ëœ ëª¨ë‹¬ë¦¬í‹° í™•ì¸
  private getUsedModalities(input: MultimodalInput): string[] {
    const modalities: string[] = [];
    
    if (input.text) modalities.push('text');
    if (input.audio) modalities.push('audio');
    if (input.image) modalities.push('image');
    
    return modalities;
  }

  // ìŒì„± í•©ì„±
  async synthesizeSpeech(text: string, language: 'ko' | 'en' = 'ko'): Promise<void> {
    if (!this.speechSynthesis) return;

    const utterance = new SpeechSynthesisUtterance(text);
    utterance.lang = language === 'ko' ? 'ko-KR' : 'en-US';
    utterance.rate = 0.9;
    utterance.pitch = 1.0;
    utterance.volume = 0.8;

    this.speechSynthesis.speak(utterance);
  }

  // ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì‹œì‘
  startListening(callback: (transcript: string, isFinal: boolean) => void): void {
    if (!this.speechRecognition || this.isListening) return;

    this.isListening = true;
    this.speechRecognition.continuous = true;
    this.speechRecognition.interimResults = true;

    this.speechRecognition.onresult = (event) => {
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        const isFinal = event.results[i].isFinal;
        callback(transcript, isFinal);
      }
    };

    this.speechRecognition.onerror = (event) => {
      console.error('Speech recognition error:', event.error);
      this.isListening = false;
    };

    this.speechRecognition.onend = () => {
      this.isListening = false;
    };

    this.speechRecognition.start();
  }

  // ìŒì„± ì¸ì‹ ì¤‘ì§€
  stopListening(): void {
    if (this.speechRecognition && this.isListening) {
      this.speechRecognition.stop();
      this.isListening = false;
    }
  }

  // ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
  getServiceStatus(): {
    speechRecognition: boolean;
    speechSynthesis: boolean;
    imageProcessing: boolean;
  } {
    return {
      speechRecognition: !!this.speechRecognition,
      speechSynthesis: !!this.speechSynthesis,
      imageProcessing: true // í•­ìƒ ì‚¬ìš© ê°€ëŠ¥
    };
  }
}

export const multimodalAIService = new MultimodalAIService();
export default multimodalAIService;